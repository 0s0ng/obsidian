# 1. 기반 아키텍처

![[Pasted image 20251218142140.png]]

# 2. VM 환경 구축(Vagrant)

Vagrant를 이용하여 위의 아키텍처에 기반해 필요한 서버를 빠르게 생성합니다.
이 문서는 Windows 환경 기반으로 작성된 핸즈온 문서입니다.

## 2.1 Vagrantfile 만들기

1. `vagrant init` 입력
2. `Vagrantfile` 파일을 다음과 같이 수정

```Plain text
# -*- mode: ruby -*-
# vi: set ft=ruby :

# common variables
BOX_NAME = "nobreak-labs/rocky-9"
MEMORY = "1024"
CPUS = 2

# IP config
NODES = {
  "WP-LB01" => { 
    networks: [
      { ip: "192.168.56.10" },
      { ip: "192.168.57.10" }
    ]
  },
  "WP-WEB01" => { 
    networks: [
      { ip: "192.168.57.11" },
      { ip: "192.168.58.11" }
    ]
  },
  "WP-WEB02" => { 
    networks: [
      { ip: "192.168.57.12" },
      { ip: "192.168.58.12" }
    ]
  },
  "WP-DB01" => { 
    networks: [
      { ip: "192.168.58.13" }
    ]
  },
  "WP-NFS01" => { 
    networks: [
      { ip: "192.168.57.14" }
    ],
    disk: { size: "10GB", name: "sdb" }
  },
  "WP-iSCSI01" => { 
    networks: [
      { ip: "192.168.58.15" }
    ],
    disk: { size: "10GB", name: "sdb" }
  }
}

Vagrant.configure("2") do |config|
  
  NODES.each do |node_name, node_config|
    config.vm.define node_name do |node|
      node.vm.box = BOX_NAME
      
      # Network config
      node_config[:networks].each do |network|
        node.vm.network "private_network", ip: network[:ip]
      end
      
      node.vm.synced_folder ".", "/vagrant", disabled: true
      
      node.vm.provider "virtualbox" do |vb|
        vb.memory = MEMORY
        vb.cpus = CPUS
        vb.name = node_name
      end
      
      node.vm.hostname = node_name
      
      # Add disk
      if node_config[:disk]
        node.vm.disk :disk, size: node_config[:disk][:size], name: node_config[:disk][:name]
      end
    end
  end
end
```

> 위의 [Vagrantfile](https://gist.github.com/Seop0728/572c5aaa341ab4d50e27526f271cdbfd)은 성민섭 연구원님의 GitHub에서 가져왔습니다.

## 2.2 VM 만들기 및 실행

- `vagrant up` 입력
	- .vagrant 폴더 생성됨
	- Vagrant 파일에서 설정한 VM들이 만들어지고 실행됨

## 2.3 구축 순서

**iSCSI -> DB -> NFS -> WEB -> LB**

의존성을 고려해 데이터가 저장되는 스토리지, DB부터 상향식 구축

- 하향식 구축(Top-Down)
	- 인터페이스 중심
	- 아키텍처의 전체적인 흐름을 먼저 정의할 수 있음
	- 대규모 프로젝트에서는 HAProxy 설정 담당자와 DB 담당자가 동시에 작업할 수 있음(위와 같이 아키텍처 설계가 되어있다는 가정 하)
	- 대신 하위 시스템이 준비될 때까지 실제 서비스 동작을 확인할 수 없음
	- 트래픽 흐름을 테스트하기 좋음
- 상향식 구축(Bottom-Up)
	- 의존성 중심
	- 검증하기 용이함
	- 각 서버 구축이 완료될 때 마다 바로 상위 계층을 붙여서 테스트할 수 있음
	- 문제가 생기면 직관적으로 어디에 설정이 잘못되었는지 알 수 있음
	- 데이터 무결성을 확보하며 올라오기 좋음

# 3. iSCSI 서버 구축
## 3.1 iSCSI 서버 접속
```
vagrant ssh WP-iSCSI01
```

## 3.2 iSCSI 패키지 설치 및 활성화
```
# iSCSI 서버 패키지 다운
sudo dnf install -y targetcli

# target 즉시 활성화
sudo systemctl enable --now target

# target 서비스 활성화 확인 (active인지 확인)
sudo systemctl is-active target

# target 서비스 부팅 시 자동실행 설정 확인 (enabled인지 확인)
sudo systemctl is-enabled target
```
- target: 블록 장치를 공유하는 스토리지 서버 (iSCSI 서버)
- **is-active** vs **is-enabled**
	- is-active: 현재 해당 서비스가 돌아가고 있는지 확인
	- is-enabled: 부팅 시 자동 실행 설정이 잘 되어있는지 확인

## 3.3 공유 블록 생성
```
# 1. fileio 방식
## fileio 스토리지 오브젝트 생성을 위한 디렉토리 생성
# sudo mkdir /WP-iSCSI01

## targetcli 진입
# sudo targetcli

## 공유할 블록 생성
# backstores/fileio create name=disk1 file_or_dev=/WP-iSCSI01/iscsi_disk1 size=1G

## 블록 생성 확인
# ls
########################################################################
# 2. block 방식
# lsblk 로 확인했을 때 공유되지 않은 빈 디스크가 존재하는지 확인

# targetcli 진입
sudo targetcli

# sdb 디스크 전체를 iSCSI_disk1 로 등록
backstores/block create name=iSCSI_disk1 dev=/dev/sdb
```
- iSCSI 저장소 유형
	- fileio: 로컬 파일 시스템에서 일반 파일을 디스크 이미지로 사용
	- block: 로컬 블록 장치 및 논리적 장치를 사용
	- pscsi: 스토리지 오브젝트 SCSI 명령의 직접 패스스루를 지원
	- ramdisk: 임시 RAM 백업 장치를 생성

## 3.4 서버 주소 생성 및 연결
```
sudo targetcli

################### 1. fileio 방식 시작#####################
# 서버 주소 생성: 고유 키 값 생성
# cd iscsi
# create iqn.2025-12.com.iscsi:wp-iscsi01.target
# ls #잘 만들어졌는지 확인
#
# ACL 생성: iSCSI와 연결할 DB 서버 이름 예약
# cd iqn.2025-12.com.iscsi:wp-iscsi01.target/tpg1/acls
# create iqn.2025-12.com.iscsi:wp-db01.initiator
#
# LUN 생성: 미리 만든 가상 디스크 파일과 연결
# cd ../luns
# create /backstores/fileio/disk1
#
# 설정 저장 및 확인
# ls # 이상 없으면
# saveconfig # 설정 수동 저장
# exit # targetcli 접속 종료
# exit # iSCSI 접속 종료
#################### 1. fileio 방식 끝 ######################

# 2. block 방식
# iscsi target 주소 생성
iscsi create iqn.2025-12.com.iscsi:wp-iscsi01.target

# ACL 생성
cd iqn.2025-12.com.iscsi:wp-iscsi01.target/tpg1/acls
create iqn.2025-12.com.iscsi:wp-db01.initiator

# LUN 생성 및 연결
cd ../luns
create /backstores/block/iSCSI_disk1

# 설정 확인 및 저장
ls /
saveconfig
exit
exit
```
- iqn(iSCSI Qualified Name): 해당 서버가 네트워크상에서 불리는 이름
	- iqn: 해당 이름은 iSCSI 표준 형식을 따르는 이름임을 선언 (prefix)
	- 2025-12: 해당 도메인 소유 또는 설정을 시작한 날짜
	- com.iscsi: 서버의 도메인 이름을 거꾸로 쓴 것
	- : 이후로는 관리자가 저장소에 붙인 별명
	- 고유 키. 수정 안됨(삭제하고 새로 생성해야함)
- LUN(Logical Unit Number)
	- 네트워크를 통해 빌려주는 가상 하드디스크 한 칸
	- 서버에 있는 실제 파일(fileio)은 물리적인 실체이고 DB서버(initiator)가 인식하게 될 가상 디스크를 LUN이라고 함
## 3.5 방화벽 설정
```
# 방화벽에서 iSCSI target 서버 포트 열어주기 (success 확인)
sudo firewall-cmd --permanent --add-service=iscsi-target
sudo firewall-cmd --reload

# 포트 잘 열렸는지 확인(3260/tcp 확인)
sudo firewall-cmd --list-ports
```
- firewall-cmd에서 service로 여는 것과 port로 여는 것의 차이
	- 포트
		- 특정 번호를 지정해서 여는 방식
		- 직관적이고 빠름. 어떤 포트가 열려있는지 번호로 바로 확인 가능
		- 왜 해당 포트가 열려있는지 까먹을 수 있음
		- command: `firewall-cmd --permanent --add-port=3260/tcp`
	- 서비스
		- 미리 정의된 설정 파일(XML)을 불러와서 관련 포트들을 한꺼번에 여는 방식
		- 한 서비스가 여러 개의 포트 사용하는 경우 한 번에 열 수있음
		- 어떤 서비스를 사용하고 있는지 바로 확인 가능하지만 어떤 포트가 열린지 바로 알 수 없음
		- 해당 서비스의 설정 파일이 시스템에 미리 등록되어 있어야함
		- path: /usr/lib/firewalld/services/service.xml
		- command: `firewall-cmd --info-service=service`

# 4. DB 서버 구축
## 4.1 DB 서버 접속
```
vagrant ssh WP-DB01
```

## 4.2 DB 서버, iSCSI initiator 패키지 설치 및 활성화
```
# iSCSI initiator, db server 패키지 설치
sudo dnf install -y iscsi-initiator-utils mysql-server

# 부팅 시 서비스 활성화 예약 및 바로 시작
sudo systemctl enable --now iscsid mysqld

# 서비스 활성화 및 부팅 시 활성화 예약 확인 
sudo systemctl is-active iscsid mysqld # active active 확인
sudo systemctl is-enabled iscsid mysqld # enabled enabled 확인
```

## 4.3 iSCSI 서버 연결
```
# db 서버의 IQN 이름 확인 (target 서버 ACL에 등록한 이름인지 확인)
cat /etc/iscsi/initiatorname.iscsi 

# target 서버 ACL에 등록한 이름과 다른 경우 이름 변경
echo "InitiatorName=iqn.2025-12.com.iscsi:wp-db01.initiator" | sudo tee /etc/iscsi/initiatorname.iscsi

# 설정 적용: 서비스 재시작
sudo systemctl restart iscsid

# iSCSI target 검색
# 결과가 나오지 않으면 방화벽이나 iSCSI 서버의 ACL 다시 확인
sudo iscsiadm -m discovery -t st -p 192.168.58.15:3260

# iSCSI Portal 로그인 (로그아웃은 -u) login in to ~ successful 출력 확인
sudo iscsiadm -m node -T iqn.2025-12.com.iscsi:wp-iscsi01.target -p 192.168.58.15 -l

# iSCSI Session 조회
sudo iscsiadm -m session -o show 
```
- `iscsiadm -m node -T iqn.2025-12.com.iscsi:wp-iscsi01.target -p 192.168.58.15 -l`
	- -m node: 노드 모드로 동작하겠다
	- -T \[target\]: 연결하려는 target의 IQN 지정
	- -p \[ip\]: target의 ip 주소 (ip:port 형식인데 포트 생략함)
	- -l: 로그인. 연결하겠다.
	- -u: 로그아웃. 연결 끊겠다. 
## 4.4 파일 시스템 생성 및 공유 디스크 마운트
```
# 디스크 확인
lsblk # sdb 존재 확인

# 파티션 나누기 생각 (굳이 할 필요 없을듯)

# 파티션 및 포맷
sudo mkfs.xfs /dev/sdb

# 마운트 전 기존 폴더 백업 파일 만들기
sudo mv /var/lib/mysql /var/lib/mysql.bak
sudo mkdir /var/lib/mysql

# 마운트
sudo mount /dev/sdb /var/lib/mysql

# 만약 이미 DB 서버 사용중 iSCSI를 마운트하는 거라면
# sudo cp -rp /var/lib/mysql.bak/* /var/lib/mysql

# 소유권 및 권한 설정
sudo chown -R mysql:mysql /var/lib/mysql
sudo chmod 751 /var/lib/mysql

# 부팅 시 자동 마운트 설정
# echo "/dev/sdb /var/lib/mysql xfs defaults,_netdev 0 0" | sudo tee -a /etc/fstab

# UUID 확인
# UUID_VAL=$(blkid -s UUID -o value /dev/sdb) echo "[해당UUID]" 
# echo "UUID=$UUID_VAL /var/lib/mysql xfs defaults,_netdev 0 0" | sudo tee -a /etc/fstab

# 실제 디스크 용량이 잡혔는지 확인
df -h /var/lib/mysql

# 보안 점검 
# sudo restorecon -Rv /var/lib/mysql

# DB 로그아웃
exit

# 서비스 재시작
systemctl restart mysql

# db 서버 로그아웃
exit
```
- XFS 사용 이유
	- 대용량 파일 및 스토리지를 위해 구성됨 -> DB 환경에 적합함
	- 병령 I/O 성능: 여러 프로세스가 동시에 디스크에 접근할 때 성능 저하가 적음
	- Rocky Linux 표준 파일 시스템임
	- 유지보수: LUN 용량을 늘렸을 때 umount 없이 즉시 용량 확장 가능(xfs_growfs)

# 5. NFS 서버 구축

NFS(Network File System) 서버는 Web 1,2 서버 간 실시간 데이터 공유 효율성과 중앙화된 데이터 무결성 보장의 이유로 구축합니다.

## 5.1 NFS 서버 접속

```PowerShell
vagrant ssh WP-NFS01
```

## 5.2 NFS 패키지 설치 및 활성화

```Bash
sudo dnf install -y nfs-utils
sudo systemctl enable --now rpcbind nfs-server
```

- `rpcbind` 서비스
	- RPC(Remote Procedure Call)는 서비스 등록/중계를 담당함
	- NFS는 고정 포트가 아닌 동적 포트를 사용하기 때문에 클라이언트가 NFS 서버의 mountd, nfsd 등의 포트를 알 수 없음
	- rpcbind가 클라이언트 요청을 받아 해당 RPC 서비스로 연결함
	- 111번 포트 사용 및 TCP와 UDP 모두 지원

## 5.3 NFS 공유 디렉토리 설정 및 내보내기

### 5.3.1 NFS 공유 디렉토리 생성

```Bash
sudo mkdir /WP-NFS01
```

### 5.3.2 NFS 공유 디렉토리 권한 설정

```Bash
sudo chmod 755 /WP-NFS01
```

- `no_root_squash`

### 5.3.3 exports 파일 설정

```Bash
echo "/WP-NFS01 192.168.57.0/24(rw,sync,no_root_squash)" | sudo tee -a /etc/exports
```

### 5.3.4 exports 파일 설정 저장 및 확인

```Bash
# 공유할 디렉토리 생성
sudo mkdir /WP-NFS01

# 권한 설정
sudo chmod 755 /WP-NFS01

# 어떤 네트워크 대역에 권한을 줄지 설정
echo "/WP-NFS01 192.168.57.0/24(rw,sync,no_root_squash)" | sudo tee -a /etc/exports

# 설정 저장
sudo exportfs -ra

# 설정 확인
sudo exportfs -v
```

- exportfs : NFS 서버에서 어떤 디렉토리를 어떤 클라이언트에 공유할지 정의하는 설정 파일
- `no_root_squash`: 클라이언트의 root 사용자를 서버에서도 root 권한으로 인정함
	- 변경하지 않고 기본값 `root_squash`를 사용할 경우
		- `sudo chown -R nobody:nobody /WP-NFS01` 설정 필요
		- 클라이언트의 root 사용자는 서버에서 nobody 권한으로 인정함
- 

## 5.4 방화벽 설정

```Bash
# 방화벽 설정
sudo firewall-cmd --permanent --add-service=nfs
sudo firewall-cmd --permanent --add-service=mountd
sudo firewall-cmd --permanent --add-service=rpc-bind
sudo firewall-cmd --reload

# NFS 서버 로그아웃
exit
```

# 6. Web1,2 서버 구축
## 6.1. Web 서버 접속
```
vagrant ssh WP-WEB01
```

## 6.2. apache, wordpress, mysql, nfs 패키지 설치
```
sudo dnf install -y httpd php php-gd mysql nfs-utils 

# 패키지 실행 및 활성화
sudo systemctl enable --now httpd

# 방화벽 설정
sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --reload
```

## 6.3. NFS 디렉토리 마운트
```
# 웹 서버의 데이터를 담을 디렉토리 생성
sudo mkdir -p /var/www/html

# NFS 서버 IP와 공유 디렉토리 연결
sudo mount -t nfs 192.168.57.14:/WP-NFS01 /var/www/html

# 부팅 시 자동 마운트 설정
echo "192.168.57.14:/WP-NFS01 /var/www/html nfs defaults 0 0" | sudo tee -a /etc/fstab
```

## 6.4. WordPress 설치 및 구성
```
# WordPress 다운 및 저장
curl -o wordpress.tar.gz https://wordpress.org/wordpress-6.8.2.tar.gz

# 압축 해제
sudo tar -xzf wordpress.tar.gz -C /var/www/html

# WordPress 설정 파일 생성 및 설정
sudo cp /var/www/html/wordpress/wp-config-sample.php /var/www/html/wordpress/wp-config.php

# 권한 변경
sudo chown -R apache:apache /var/www/html/wordpress/wp-config.php

sudo vi /var/www/html/wordpress/wp-config.php
# define( 'DB_NAME', 'WP' );
# define( 'DB_USER', 'wpus' );
# define( 'DB_PASSWORD', 'Nobreak123!' );
# define( 'DB_HOST', '192.168.58.40' );

# DocumentRoot 변경
# <VirtualHost *:80>
#     ServerName 192.168.56.10
#     DocumentRoot /var/www/html/wordpress
#     <Directory /var/www/html/wordpress>
#         AllowOverride All
#         Require all granted
#     </Directory>
# </VirtualHost>

sudo systemctl restart httpd
```
## 6.4. DB 연결 확인
```
mysql -u wpus -p -h 192.168.58.40
```

# 7. LB 구축
## 7.1. LB 서버 접속
```
vagrant ssh WP-LB01
```

## 7.2. HAProxy 패키지 설치 및 활성화
```
sudo dnf install -y haproxy

# 활성화
sudo systemctl enable --now haproxy
```

## 7.3. 커널 파라미터 최적화
```
# IP 포워딩 및 비로컬 바인딩 허용
echo "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.conf
echo "net.ipv4.ip_nonlocal_bind = 1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

## 7.4. HAProxy 설정
```
# 기존 파일 백업
sudo cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak
sudo vi /etc/haproxy/haproxy.cfg
```

> /etc/haproxy/haproxy.cfg 파일 내부
> ```
> global 
> 	log /dev/log local0 
> 	log /dev/log local1 notice 
> 	chroot /var/lib/haproxy 
> 	stats socket /run/haproxy/admin.sock mode 660 level admin 
> 	stats timeout 30s 
> 	user haproxy 
> 	group haproxy 
> 	daemon 
> 	
> defaults 
> 	log global 
> 	mode http 
> 	option httplog 
> 	option dontlognull 
> 	timeout connect 5000 
> 	timeout client 50000 
> 	timeout server 50000 
> 	
> # 1. 모니터링 통계 페이지 (관리용) 
> listen stats 
> 	bind *:9000 
> 	stats enable 
> 	stats uri /haproxy_stats 
> 	stats auth admin:password123 # 실제 운영 시 변경 필수 
> 	
> # 2. 클라이언트가 접속하는 지점 (Frontend) 
> frontend http-in 
> 	bind 192.168.56.10:80 
> 	# X-Forwarded-For 헤더 추가 (웹서버에서 실제 클라이언트 IP 확인용) 
> 	option forwardfor 
> 	default_backend wp-cluster 
> 
> # 3. 실제 부하가 분산될 서버 군 (Backend) 
> backend wp-cluster 
> 	balance roundrobin 
> 	# WordPress 세션 유지를 위한 쿠키 설정 
> 	cookie SERVERID insert indirect nocache 
> 	
> 	# 서버 상태 체크(check) 포함 
> 	server web1 192.168.57.11:80 check cookie s1 
> 	server web2 192.168.57.12:80 check cookie s2
> ```

```
# 설정 검증
# "Configuration file is valid" 메시지가 나오면 성공
sudo haproxy -c -f /etc/haproxy/haproxy.cfg
```

## 7.5. 방화벽 설정
```
sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=9000/tcp
sudo firewall-cmd --reload
```